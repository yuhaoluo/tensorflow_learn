{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from DataReader import FeatureDictionary, DataParser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config\n",
    "#from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "#pip install pandas -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "#folds = list(StratifiedKFold(n_splits=config.NUM_SPLITS, shuffle=True,\n",
    "#                            random_state=config.RANDOM_SEED).split(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFM(keras.Model):\n",
    "    def __init__(self,feature_size,field_size,embedding_size,dropout_rate=0.2):\n",
    "        super(NFM,self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.field_size = field_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.loss_type = 'logloss'\n",
    "        \n",
    "        ## init weights\n",
    "        ## globle bias\n",
    "        self.bias = tf.Variable(tf.constant([0.1]),name='glable_bias')\n",
    "\n",
    "        self.feature_embeddings = tf.Variable(\n",
    "            tf.random.normal([self.feature_size,self.embedding_size],0.0,0.01),\n",
    "            name='weight_embeddings')\n",
    "        \n",
    "        #self.feature_embeddings = layers.Embedding(self.feature_size,self.embedding_size,\n",
    "        #                                           embeddings_initializer='uniform')\n",
    "        self.weight_first = tf.Variable(tf.random.normal([self.feature_size,1],0.0,1.0),name='weight_firstorder')\n",
    "        \n",
    "  \n",
    "        ##\n",
    "        self.dense_1 = layers.Dense(32,activation='relu')\n",
    "        self.dense_2 = layers.Dense(32,activation='relu')\n",
    "        self.dense_out = layers.Dense(1,use_bias=False)\n",
    "        \n",
    "    def call(self,inputs_index,inputs_value):\n",
    "        ## calc embedding\n",
    "        self.embedding = tf.nn.embedding_lookup(self.feature_embeddings,inputs_index)\n",
    "        inputs_value = tf.reshape(inputs_value,[-1,self.field_size,1])\n",
    "        self.embedding = tf.multiply(self.embedding,inputs_value)  ## (N,F,K)\n",
    "        #print('self.embedding',self.embedding)\n",
    "        \n",
    "        ## first order term\n",
    "        first_order = tf.nn.embedding_lookup(self.weight_first,inputs_index)  ## (N,F,1)\n",
    "        first_order = tf.reduce_sum(tf.multiply(first_order,inputs_value),axis=2)  ##(N,F)\n",
    "        #print('first_order*inputs_value',first_order)\n",
    "        \n",
    "        ## second order term \n",
    "        sum_feature_emb = tf.reduce_sum(self.embedding,axis=1)  ##(N,K)\n",
    "        sum_feature_emb_squre = tf.square(sum_feature_emb)\n",
    "        #print('sum_feature_emb',sum_feature_emb)\n",
    "        #print('sum_feature_emb_squre',sum_feature_emb_squre)\n",
    "        square_feature_emb = tf.square(self.embedding)\n",
    "        square_feature_emb_sum = tf.reduce_sum(square_feature_emb,axis=1) ##(N,K)\n",
    "        #print('square_feature_emb',square_feature_emb)\n",
    "        #print('square_feature_emb_sum',square_feature_emb_sum)\n",
    "        \n",
    "        second_order = 0.5 * tf.subtract(sum_feature_emb_squre,square_feature_emb_sum)\n",
    "        #print('second_order',second_order)\n",
    "\n",
    "        second_order = tf.reshape(second_order,[-1,self.embedding_size]) ##(N,K)\n",
    "        #print('second_order',second_order)\n",
    "        deep_feature = self.dense_1(second_order)\n",
    "        layers.Dropout(self.dropout_rate)\n",
    "        deep_feature = self.dense_2(deep_feature)\n",
    "        layers.Dropout(self.dropout_rate)\n",
    "        deep_out = self.dense_out(deep_feature)  ##(N,1)\n",
    "  \n",
    "        out = tf.add_n([tf.reduce_sum(first_order,axis=1,keepdims=True),\n",
    "                             deep_out,\n",
    "                             self.bias * tf.ones_like(deep_out)],name='out_nfm')\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dfTrain = pd.read_csv(config.TRAIN_FILE)\n",
    "    dfTest = pd.read_csv(config.TEST_FILE)\n",
    "\n",
    "    def preprocess(df):\n",
    "        cols = [c for c in df.columns if c not in ['id','target']]\n",
    "        #df['missing_feat'] = np.sum(df[df[cols]==-1].values,axis=1)\n",
    "        df[\"missing_feat\"] = np.sum((df[cols] == -1).values, axis=1)\n",
    "        df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "        return df\n",
    "\n",
    "    dfTrain = preprocess(dfTrain)\n",
    "    dfTest = preprocess(dfTest)\n",
    "\n",
    "    cols = [c for c in dfTrain.columns if c not in ['id','target']]\n",
    "    cols = [c for c in cols if (not c in config.IGNORE_COLS)]\n",
    "\n",
    "    X_train = dfTrain[cols].values\n",
    "    y_train = dfTrain['target'].values\n",
    "\n",
    "    X_test = dfTest[cols].values\n",
    "    ids_test = dfTest['id'].values\n",
    "\n",
    "    cat_features_indices = [i for i,c in enumerate(cols) if c in config.CATEGORICAL_COLS]\n",
    "\n",
    "    return dfTrain,dfTest,X_train,y_train,X_test,ids_test,cat_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dfTrain, dfTest, X_train, y_train, X_test, ids_test, cat_features_indices = load_data()\n",
    "#dfTrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (10000, 39)\n",
      "X_test: (2000, 39)\n"
     ]
    }
   ],
   "source": [
    "print('X_train:',X_train.shape)\n",
    "print('X_test:',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/DataReader.py:36: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  df = pd.concat([dfTrain,dfTest])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_size 256\n",
      "field_size 39\n",
      "embedding_size 8\n"
     ]
    }
   ],
   "source": [
    "fd = FeatureDictionary(dfTrain=dfTrain,\n",
    "                           dfTest=dfTest,\n",
    "                           numeric_cols=config.NUMERIC_COLS,\n",
    "                           ignore_cols = config.IGNORE_COLS)\n",
    "data_parser = DataParser(feat_dict= fd)\n",
    "# Xi_train ：列的序号\n",
    "# Xv_train ：列的对应的值\n",
    "Xi_train,Xv_train,y_train = data_parser.parse(df=dfTrain,has_label=True)\n",
    "Xi_test,Xv_test,ids_test = data_parser.parse(df=dfTest)\n",
    "\n",
    "#print(dfTrain.dtypes)\n",
    "nfm_params = dict()\n",
    "embedding_size = 8\n",
    "nfm_params['feature_size'] = fd.feat_dim\n",
    "nfm_params['field_size'] = len(Xi_train[0])\n",
    "nfm_params['embedding_size'] = embedding_size\n",
    "print('feature_size',fd.feat_dim)\n",
    "print('field_size',len(Xi_train[0]))\n",
    "print('embedding_size',embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfm_model = NFM(**nfm_params)\n",
    "learning_rate = 0.001\n",
    "#optimizer = tf.optimizers.Adam(learning_rate)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfm_loss(y_pred,y,loss_type='logloss'): \n",
    "    #print(loss)\n",
    "    if loss_type == 'logloss':\n",
    "        y_pred = tf.reshape(tf.nn.sigmoid(y_pred),[-1,1])\n",
    "        y = tf.reshape(y,[-1,1])\n",
    "        loss = tf.compat.v1.losses.log_loss(y,y_pred)\n",
    "    elif loss_type == 'mse':\n",
    "        loss = tf.reduce_mean(tf.keras.losses.MSE(y_true=y,y_pred=y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_x,train_y):\n",
    "    with tf.GradientTape() as tape:  \n",
    "        x_i, x_v  = train_x\n",
    "        y_pred = nfm_model(x_i,x_v)\n",
    "        loss = nfm_loss(y_pred,train_y)\n",
    "        \n",
    "    train_vars =  nfm_model.trainable_variables\n",
    "    grad = tape.gradient(loss,train_vars)\n",
    "    adam.apply_gradients(zip(grad,train_vars))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y):\n",
    "    y_pred = tf.cast(y_pred > 0.5,dtype=tf.int32)\n",
    "    pre_cls = tf.equal(y_pred,tf.cast(y,tf.int32))\n",
    "    acc = tf.reduce_mean(tf.cast(pre_cls,tf.float32))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi_val = Xi_train[-2000:]\n",
    "Xv_val = Xv_train[-2000:]\n",
    "y_val = y_train[-2000:]\n",
    "\n",
    "Xi_train = Xi_train[:8000]\n",
    "Xv_train = Xv_train[:8000]\n",
    "y_train = y_train[:8000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_data = tf.data.Dataset.from_tensor_slices(((Xi_train,Xv_train),y_train))\n",
    "train_data = train_data.shuffle(8000).repeat().batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_epoch: 1, loss: 0.110856, acc: 0.960500\n",
      "_epoch: 2, loss: 0.110391, acc: 0.960500\n",
      "_epoch: 3, loss: 0.111099, acc: 0.960500\n",
      "_epoch: 4, loss: 0.111154, acc: 0.960500\n",
      "_epoch: 5, loss: 0.109618, acc: 0.960500\n",
      "_epoch: 6, loss: 0.107781, acc: 0.959579\n",
      "_epoch: 7, loss: 0.107024, acc: 0.960039\n",
      "_epoch: 8, loss: 0.106472, acc: 0.959579\n",
      "_epoch: 9, loss: 0.104882, acc: 0.960039\n",
      "_epoch: 10, loss: 0.106376, acc: 0.960500\n",
      "_epoch: 11, loss: 0.108658, acc: 0.960500\n",
      "_epoch: 12, loss: 0.104184, acc: 0.960500\n",
      "_epoch: 13, loss: 0.112681, acc: 0.957277\n",
      "_epoch: 14, loss: 0.103700, acc: 0.960500\n",
      "_epoch: 15, loss: 0.115106, acc: 0.960500\n",
      "_epoch: 16, loss: 0.103015, acc: 0.960500\n",
      "_epoch: 17, loss: 0.104412, acc: 0.959579\n",
      "_epoch: 18, loss: 0.102783, acc: 0.960039\n",
      "_epoch: 19, loss: 0.104082, acc: 0.960500\n",
      "_epoch: 20, loss: 0.103318, acc: 0.960039\n",
      "_epoch: 21, loss: 0.099593, acc: 0.959579\n",
      "_epoch: 22, loss: 0.099755, acc: 0.960500\n",
      "_epoch: 23, loss: 0.103472, acc: 0.960500\n",
      "_epoch: 24, loss: 0.098295, acc: 0.958197\n",
      "_epoch: 25, loss: 0.098079, acc: 0.959118\n",
      "_epoch: 26, loss: 0.101291, acc: 0.960500\n",
      "_epoch: 27, loss: 0.097115, acc: 0.959118\n",
      "_epoch: 28, loss: 0.096757, acc: 0.960039\n",
      "_epoch: 29, loss: 0.095694, acc: 0.959579\n",
      "_epoch: 30, loss: 0.094096, acc: 0.959118\n",
      "_epoch: 31, loss: 0.094787, acc: 0.958197\n",
      "_epoch: 32, loss: 0.094996, acc: 0.959579\n",
      "_epoch: 33, loss: 0.093192, acc: 0.958197\n",
      "_epoch: 34, loss: 0.094907, acc: 0.956356\n",
      "_epoch: 35, loss: 0.092343, acc: 0.959579\n",
      "_epoch: 36, loss: 0.092365, acc: 0.958658\n",
      "_epoch: 37, loss: 0.093226, acc: 0.959579\n",
      "_epoch: 38, loss: 0.090602, acc: 0.957737\n",
      "_epoch: 39, loss: 0.090267, acc: 0.957277\n",
      "_epoch: 40, loss: 0.091294, acc: 0.955435\n",
      "_epoch: 41, loss: 0.090351, acc: 0.957277\n",
      "_epoch: 42, loss: 0.088915, acc: 0.956356\n",
      "_epoch: 43, loss: 0.090240, acc: 0.954513\n",
      "_epoch: 44, loss: 0.088648, acc: 0.958197\n",
      "_epoch: 45, loss: 0.087694, acc: 0.958658\n",
      "_epoch: 46, loss: 0.089185, acc: 0.959118\n",
      "_epoch: 47, loss: 0.087602, acc: 0.957277\n",
      "_epoch: 48, loss: 0.086478, acc: 0.958197\n",
      "_epoch: 49, loss: 0.086402, acc: 0.955435\n",
      "_epoch: 50, loss: 0.085433, acc: 0.958658\n"
     ]
    }
   ],
   "source": [
    "## RUN Training\n",
    "display_step = 200\n",
    "steps_per_epoch = int(len(Xi_train) / batch_size)\n",
    "epoch = 50\n",
    "training_steps = epoch * steps_per_epoch\n",
    "\n",
    "_epoch = 0\n",
    "for step , (batch_x,batch_y) in enumerate(train_data.take(training_steps),1):\n",
    "    train_step(batch_x,batch_y) \n",
    "    if step % steps_per_epoch == 0:\n",
    "        y_pred = nfm_model(Xi_val,Xv_val)\n",
    "        cur_loss = nfm_loss(y_pred,y_val)\n",
    "        acc = accuracy(y_pred,y_val)\n",
    "        _epoch += 1\n",
    "        print(\"_epoch: %i, loss: %f, acc: %f\" % (_epoch, cur_loss,acc))\n",
    "#         x_i, x_v = batch_x\n",
    "#         y_pred = nfm_model(x_i,x_v)\n",
    "#         cur_loss = nfm_loss(y_pred,batch_y)\n",
    "#         acc = accuracy(y_pred,batch_y)\n",
    "#         print(\"step: %i, loss: %f, acc: %f\" % (step, cur_loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = nfm_model(Xi_val,Xv_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nfm_model.save_weights()\n",
    "\n",
    "print(model.feature_embeddings)\n",
    "print(model.weight_first)\n",
    "\n",
    "test_embeddings = layers.Embedding(5,3,embeddings_initializer='uniform')\n",
    "#test_embeddings.numpy()\n",
    "test_embeddings\n",
    "\n",
    "index = tf.constant([[0,1]],dtype = tf.int32)\n",
    "print(test_embeddings(index))\n",
    "\n",
    "index = tf.constant([[0,1],[1,2]],dtype = tf.int32)\n",
    "print(test_embeddings(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
