{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from DataReader import FeatureDictionary, DataParser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFM(keras.Model):\n",
    "    def __init__(self,feature_size,field_size,embedding_size,dropout_rate=0.2):\n",
    "        super(NFM,self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.field_size = field_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.loss_type = 'logloss'\n",
    "        \n",
    "        ## init weights\n",
    "        ## globle bias\n",
    "        self.bias = tf.Variable(tf.constant([0.1]),name='glable_bias')\n",
    "\n",
    "        self.feature_embeddings = tf.Variable(\n",
    "            tf.random.normal([self.feature_size,self.embedding_size],0.0,0.01),\n",
    "            name='weight_embeddings')\n",
    "        \n",
    "        #self.feature_embeddings = layers.Embedding(self.feature_size,self.embedding_size,\n",
    "        #                                           embeddings_initializer='uniform')\n",
    "        self.weight_first = tf.Variable(tf.random.normal([self.feature_size,1],0.0,1.0),name='weight_firstorder')\n",
    "        \n",
    "  \n",
    "        ##\n",
    "        self.dense_1 = layers.Dense(32,activation='relu')\n",
    "        self.dense_2 = layers.Dense(32,activation='relu')\n",
    "        self.dense_out = layers.Dense(1,use_bias=False)\n",
    "        \n",
    "    def call(self,inputs_index,inputs_value):\n",
    "        ## calc embedding\n",
    "        self.embedding = tf.nn.embedding_lookup(self.feature_embeddings,inputs_index)\n",
    "        inputs_value = tf.reshape(inputs_value,[-1,self.field_size,1])\n",
    "        self.embedding = tf.multiply(self.embedding,inputs_value)  ## (N,F,K)\n",
    "        #print('self.embedding',self.embedding)\n",
    "        \n",
    "        ## first order term\n",
    "        first_order = tf.nn.embedding_lookup(self.weight_first,inputs_index)  ## (N,F,1)\n",
    "        first_order = tf.reduce_sum(tf.multiply(first_order,inputs_value),axis=2)  ##(N,F)\n",
    "        #print('first_order*inputs_value',first_order)\n",
    "        \n",
    "        ## second order term \n",
    "        sum_feature_emb = tf.reduce_sum(self.embedding,axis=1)  ##(N,K)\n",
    "        sum_feature_emb_squre = tf.square(sum_feature_emb)\n",
    "        #print('sum_feature_emb',sum_feature_emb)\n",
    "        #print('sum_feature_emb_squre',sum_feature_emb_squre)\n",
    "        square_feature_emb = tf.square(self.embedding)\n",
    "        square_feature_emb_sum = tf.reduce_sum(square_feature_emb,axis=1) ##(N,K)\n",
    "        #print('square_feature_emb',square_feature_emb)\n",
    "        #print('square_feature_emb_sum',square_feature_emb_sum)\n",
    "        \n",
    "        second_order = 0.5 * tf.subtract(sum_feature_emb_squre,square_feature_emb_sum)\n",
    "        #print('second_order',second_order)\n",
    "\n",
    "        second_order = tf.reshape(second_order,[-1,self.embedding_size]) ##(N,K)\n",
    "        #print('second_order',second_order)\n",
    "        deep_feature = self.dense_1(second_order)\n",
    "        layers.Dropout(self.dropout_rate)\n",
    "        deep_feature = self.dense_2(deep_feature)\n",
    "        layers.Dropout(self.dropout_rate)\n",
    "        deep_out = self.dense_out(deep_feature)  ##(N,1)\n",
    "  \n",
    "        out = tf.add_n([tf.reduce_sum(first_order,axis=1,keepdims=True),\n",
    "                             deep_out,\n",
    "                             self.bias * tf.ones_like(deep_out)],name='out_nfm')\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dfTrain = pd.read_csv(config.TRAIN_FILE)\n",
    "    dfTest = pd.read_csv(config.TEST_FILE)\n",
    "\n",
    "    def preprocess(df):\n",
    "        cols = [c for c in df.columns if c not in ['id','target']]\n",
    "        #df['missing_feat'] = np.sum(df[df[cols]==-1].values,axis=1)\n",
    "        df[\"missing_feat\"] = np.sum((df[cols] == -1).values, axis=1)\n",
    "        df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "        return df\n",
    "\n",
    "    dfTrain = preprocess(dfTrain)\n",
    "    dfTest = preprocess(dfTest)\n",
    "\n",
    "    cols = [c for c in dfTrain.columns if c not in ['id','target']]\n",
    "    cols = [c for c in cols if (not c in config.IGNORE_COLS)]\n",
    "\n",
    "    X_train = dfTrain[cols].values\n",
    "    y_train = dfTrain['target'].values\n",
    "\n",
    "    X_test = dfTest[cols].values\n",
    "    ids_test = dfTest['id'].values\n",
    "\n",
    "    cat_features_indices = [i for i,c in enumerate(cols) if c in config.CATEGORICAL_COLS]\n",
    "\n",
    "    return dfTrain,dfTest,X_train,y_train,X_test,ids_test,cat_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dfTrain, dfTest, X_train, y_train, X_test, ids_test, cat_features_indices = load_data()\n",
    "#dfTrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (10000, 39)\n",
      "X_test: (2000, 39)\n"
     ]
    }
   ],
   "source": [
    "print('X_train:',X_train.shape)\n",
    "print('X_test:',X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/DataReader.py:36: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  df = pd.concat([dfTrain,dfTest])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_size 256\n",
      "field_size 39\n",
      "embedding_size 8\n"
     ]
    }
   ],
   "source": [
    "fd = FeatureDictionary(dfTrain=dfTrain,\n",
    "                           dfTest=dfTest,\n",
    "                           numeric_cols=config.NUMERIC_COLS,\n",
    "                           ignore_cols = config.IGNORE_COLS)\n",
    "data_parser = DataParser(feat_dict= fd)\n",
    "# Xi_train ：列的序号\n",
    "# Xv_train ：列的对应的值\n",
    "Xi_train,Xv_train,y_train = data_parser.parse(df=dfTrain,has_label=True)\n",
    "Xi_test,Xv_test,ids_test = data_parser.parse(df=dfTest)\n",
    "\n",
    "#print(dfTrain.dtypes)\n",
    "nfm_params = dict()\n",
    "embedding_size = 8\n",
    "nfm_params['feature_size'] = fd.feat_dim\n",
    "nfm_params['field_size'] = len(Xi_train[0])\n",
    "nfm_params['embedding_size'] = embedding_size\n",
    "print('feature_size',fd.feat_dim)\n",
    "print('field_size',len(Xi_train[0]))\n",
    "print('embedding_size',embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfm_model = NFM(**nfm_params)\n",
    "learning_rate = 0.001\n",
    "#optimizer = tf.optimizers.Adam(learning_rate)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfm_loss(y_pred,y,loss_type='logloss'): \n",
    "    #print(loss)\n",
    "    if loss_type == 'logloss':\n",
    "        y_pred = tf.reshape(tf.nn.sigmoid(y_pred),[-1,1])\n",
    "        y = tf.reshape(y,[-1,1])\n",
    "        loss = tf.compat.v1.losses.log_loss(y,y_pred)\n",
    "    elif loss_type == 'mse':\n",
    "        loss = tf.reduce_mean(tf.keras.losses.MSE(y_true=y,y_pred=y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_x,train_y):\n",
    "    with tf.GradientTape() as tape:  \n",
    "        x_i, x_v  = train_x\n",
    "        y_pred = nfm_model(x_i,x_v)\n",
    "        loss = nfm_loss(y_pred,train_y)\n",
    "        \n",
    "    train_vars =  nfm_model.trainable_variables\n",
    "    grad = tape.gradient(loss,train_vars)\n",
    "    adam.apply_gradients(zip(grad,train_vars))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y):\n",
    "    y_pred = tf.cast(y_pred > 0.5,dtype=tf.int32)\n",
    "    pre_cls = tf.equal(y_pred,tf.cast(y,tf.int32))\n",
    "    acc = tf.reduce_mean(tf.cast(pre_cls,tf.float32))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_data = tf.data.Dataset.from_tensor_slices(((Xi_train,Xv_train),y_train))\n",
    "train_data = train_data.shuffle(10000).repeat().batch(batch_size)\n",
    "training_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200, loss: 0.015959, acc: 0.950478\n",
      "step: 400, loss: 0.053189, acc: 0.903404\n",
      "step: 600, loss: 0.025132, acc: 0.928505\n",
      "step: 800, loss: 0.031355, acc: 0.932091\n",
      "step: 1000, loss: 0.019656, acc: 0.923111\n"
     ]
    }
   ],
   "source": [
    "## RUN Training\n",
    "display_step = 200\n",
    "for step , (batch_x,batch_y) in enumerate(train_data.take(training_steps),1):\n",
    "    train_step(batch_x,batch_y) \n",
    "    if step % display_step == 0:\n",
    "        x_i, x_v = batch_x\n",
    "        y_pred = nfm_model(x_i,x_v)\n",
    "        cur_loss = nfm_loss(y_pred,batch_y)\n",
    "        acc = accuracy(y_pred,batch_y)\n",
    "        print(\"step: %i, loss: %f, acc: %f\" % (step, cur_loss,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfm_model.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   9,   19,   28,   39,   41,   65,   87,  108,  191,  224,  249,\n",
      "        251,  264,  266,  293,  297,  305,  328,  339,  380,  384,  460,\n",
      "        518,  528,  535,  559,  610,  631,  640,  647,  688,  702,  845,\n",
      "        858,  864,  925, 1019, 1030, 1039, 1103, 1106, 1136, 1163, 1166,\n",
      "       1168, 1185, 1263, 1279, 1352, 1362, 1370, 1387, 1404, 1407, 1417,\n",
      "       1464, 1466, 1569, 1586, 1607, 1630, 1648, 1661, 1694, 1760, 1768,\n",
      "       1771, 1777, 1781, 1823, 1840, 1847, 1882, 1886, 1958, 1980, 2005,\n",
      "       2025, 2031, 2090, 2139, 2168, 2203, 2231, 2234, 2244, 2248, 2299,\n",
      "       2308, 2343, 2369, 2399, 2433, 2437, 2474, 2483, 2537, 2579, 2583,\n",
      "       2584, 2596, 2612, 2688, 2714, 2750, 2759, 2781, 2813, 2827, 2886,\n",
      "       2945, 2959, 2964, 3015, 3084, 3108, 3122, 3173, 3208, 3231, 3273,\n",
      "       3298, 3317, 3340, 3355, 3420, 3426, 3441, 3442, 3449, 3462, 3534,\n",
      "       3558, 3567, 3639, 3679, 3696, 3797, 3810, 3824, 3889, 3901, 3930,\n",
      "       3971, 3977, 3981, 4005, 4012, 4029, 4073, 4090, 4174, 4252, 4301,\n",
      "       4308, 4373, 4397, 4461, 4473, 4478, 4483, 4546, 4560, 4606, 4621,\n",
      "       4636, 4683, 4728, 4733, 4752, 4758, 4786, 4844, 4851, 4903, 4957,\n",
      "       4970, 5004, 5062, 5065, 5103, 5159, 5274, 5378, 5397, 5409, 5417,\n",
      "       5427, 5440, 5544, 5546, 5563, 5591, 5627, 5643, 5649, 5669, 5707,\n",
      "       5716, 5722, 5743, 5748, 5840, 5856, 5859, 5929, 5930, 5932, 5936,\n",
      "       5959, 5961, 6024, 6046, 6055, 6063, 6079, 6101, 6114, 6131, 6151,\n",
      "       6196, 6217, 6228, 6250, 6287, 6316, 6318, 6326, 6327, 6352, 6371,\n",
      "       6429, 6475, 6488, 6493, 6509, 6566, 6590, 6625, 6646, 6650, 6677,\n",
      "       6725, 6861, 6862, 6884, 6893, 6899, 6905, 6917, 6928, 6948, 6989,\n",
      "       7026, 7037, 7086, 7112, 7133, 7206, 7212, 7228, 7266, 7287, 7294,\n",
      "       7350, 7422, 7425, 7430, 7439, 7549, 7551, 7568, 7596, 7609, 7628,\n",
      "       7657, 7683, 7711, 7729, 7734, 7767, 7778, 7779, 7783, 7791, 7839,\n",
      "       7885, 7919, 7969, 7997, 8000, 8001, 8029, 8040, 8072, 8082, 8096,\n",
      "       8118, 8150, 8160, 8175, 8201, 8243, 8267, 8279, 8280, 8281, 8292,\n",
      "       8311, 8319, 8336, 8375, 8381, 8386, 8389, 8429, 8466, 8480, 8488,\n",
      "       8519, 8584, 8620, 8644, 8647, 8676, 8714, 8734, 8739, 8776, 8794,\n",
      "       8806, 8822, 8824, 8827, 8842, 8843, 8871, 8886, 8891, 8935, 9000,\n",
      "       9004, 9017, 9035, 9037, 9050, 9084, 9180, 9215, 9229, 9232, 9240,\n",
      "       9244, 9302, 9311, 9316, 9333, 9380, 9381, 9398, 9400, 9401, 9405,\n",
      "       9487, 9569, 9575, 9604, 9707, 9731, 9784, 9888, 9895, 9905, 9908,\n",
      "       9939, 9951, 9953, 9966, 9988]),)\n"
     ]
    }
   ],
   "source": [
    "arr_y = np.array(y_train)\n",
    "idx = np.where(arr_y)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.feature_embeddings)\n",
    "print(model.weight_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = layers.Embedding(5,3,embeddings_initializer='uniform')\n",
    "#test_embeddings.numpy()\n",
    "test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = tf.constant([[0,1]],dtype = tf.int32)\n",
    "print(test_embeddings(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = tf.constant([[0,1],[1,2]],dtype = tf.int32)\n",
    "print(test_embeddings(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.uniform([2,3]))\n",
    "v = tf.Variable(tf.random.uniform([3]))\n",
    "w_res = tf.reshape(w,[2,3,1])\n",
    "v_res = tf.reshape(v,[1,3,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tf.reduce_sum(tf.multiply(w_res,v_res),axis=2)\n",
    "res.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
