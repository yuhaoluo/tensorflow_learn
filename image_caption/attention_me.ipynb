{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.io.read_file(path)\n",
    "img = tf.image.decode_jpeg(img,channels=3)\n",
    "img = tf.image.resize(image,(200,200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "* example \n",
    "* shape(64,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttention,self)__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(uints)\n",
    "        self.W2 = tf.keras.layers.Dense(uints)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self,feature,hidden):\n",
    "        # feature shape(batch_size,timestep,feature_size)\n",
    "        # hidden shape(batch_size,hidden_size)\n",
    "        # hidden_with_time_axis(batch_size,1,hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden,1)\n",
    "        \n",
    "        #score (batch,timestep,units) timestep=64\n",
    "        score = tf.nn.tanh(self.W1(feature) + self.W2(hidden_with_time_axis))\n",
    "        #score (batch,timestep,1)\n",
    "        score = self.V(score)\n",
    "        attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        context_vector = attention_weights * feature\n",
    "        context_vector = tf.reduce_sum(context_vector,axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self,embedding_dim):\n",
    "        super(CNN_Encoder,self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "    def call(self,x)\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self,embedding_dim,units,vocab_size):\n",
    "        super(RNN_Decoder,self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru  = tf.keras.layers.GRU(self.units,return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.Dense(self.units)\n",
    "        self.fc2 = tf.keras.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        \n",
    "    def call(self,x,features,hidden):\n",
    "        context_vector, attention_weights = self.attention(features,hidden)\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector,axis=1),x],axis=-1)\n",
    "        gru_output, gru_state = self.gru(x)\n",
    "        \n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(gru_output)\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x,(-1,x.shape[2]))\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "        return x, gru_state,attention_weights\n",
    "        \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real,pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    loss_ = loss_object(real,pred)\n",
    "    mask = tf.cast(mask,dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image_tensor,target):\n",
    "    loss = 0\n",
    "    \n",
    "    ## initial hidden state\n",
    "    hidden = decoder.reset_state(target.shape[0])\n",
    "    # shape (batch,1)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0],1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(image_tensor)\n",
    "        for i in range(1,target.shape[1]):\n",
    "            predictions,hidden,_ = decoder(dec_input,features,hidden)\n",
    "            loss += loss_function(target[:,i],predictions)\n",
    "            dec_input = tf.expand_dims(target[:,i],1)\n",
    "        \n",
    "    total_loss = (loss/ int(target.shape(1)))\n",
    "    \n",
    "    trainable_variables = encoder.trainable_variables =  decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss,trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return loss,total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(start_epoch,EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch,(image_tensor,target) in enumerate(dataset)):\n",
    "        batch_loss, t_loss = train_step(image_tensor,target)\n",
    "        total_loss += t_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
